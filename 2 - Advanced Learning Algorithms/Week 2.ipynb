{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# Outline of Notebook\n",
    "- ### Training Neural Networks\n",
    "- ### Activation Functions\n",
    "- ### Multiclass Classification (Softmax Regression)\n",
    "- ### Adam Algorithm (Faster than Gradient Descent)\n",
    "- ### Additional Layer Types\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to train it in Tensorflow:\n",
    "\n",
    "![](2022-07-21-21-33-53.png)\n",
    "- The first step is to tell the code the architecture of the neural network\n",
    "- Then, you compile the neural network and specify the loss function it should use\n",
    "- Finally, you train the neural network using some learning algorithm (Ex. gradient descent)\n",
    "- Note: epochs = the # of times the learning algorithm/gradient descent has to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different loss functions to choose and it depends on what you want to do. \n",
    "- The BinaryCrossentropy() one is the same as the logistic regression loss function; this is because we're doing binary classification\n",
    "- However, if you want to do regression using neural networks, you can use the MeanSquaredError() loss function, the same as linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Neural Networks Cost Function:</u> $$J(W, B) = \\frac{1}{N}\\sum_{i = 1}^N(L(f(\\vec{x}^{(i)}), y^{(i)})$$\n",
    "Where $W$ = all the $w$ parameters for every neuron and $B$ = all the $b$ parameters for every neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Gradient Descent for Neural Networks:</u>\n",
    "\n",
    "repeat via simultaneous updates {\n",
    "\n",
    "$w_j^{[l]} = w_j^{[l]} - \\alpha \\frac{\\partial}{\\partial w_j} J(\\vec{w}, b)$\n",
    "    \n",
    "$b_j^{[l]} = b_j^{[l]} - \\alpha \\frac{\\partial}{\\partial b_j} J(\\vec{w}, b)$\n",
    "\n",
    "}\n",
    "\n",
    "Note: $l$ stands for the layer and $j$ signifies the neuron in that layer\n",
    "\n",
    "Usually, in neural network training, the partial derivatives are obtained by using a technique called \"backpropagation\". But, all of that can be done for you by the command -- model.fit(x, y, epochs = 100) -- in tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid Activation Function: $g(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "- Linear Activation Function: $g(z) = z$\n",
    "\n",
    "- RelU: $g(z) = \\max(0, z)$\n",
    "\n",
    "NOTE: There are other activation functions but the majority of applications use the above..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to choose activation functions for different neurons:\n",
    "- Output Layer\n",
    "    - If you're doing binary classification, choose sigmoid function\n",
    "        - Tumor malignant or not\n",
    "    - If you're doing regression and you can predict both positive and negative values, choose the Linear Function\n",
    "        - Predict change in stock price of Microsoft tomorrow\n",
    "    - If you're doing regression and you can predict only positive values, choose the RelU Function\n",
    "        - Predict house prices\n",
    "- Hidden Layer(s)\n",
    "    - Most common choice: Use RelU because researchers have said that it allows the model to train faster\n",
    "    - Sigmoid function not used because it makes gradient descent slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why you can't use the Linear Activation Function everywhere?\n",
    "- If we do that, then the neural network won't be able to do anything more complicated than linear regression\n",
    "- This defeats the purpose of a neural network\n",
    "- If you use a Linear Activation Function everywhere and then use sigmoid in the output layer, that is logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification (Softmax Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall Logistic Regression:\n",
    "- $z = \\vec{w} \\cdot \\vec{x} + b$\n",
    "- $a_1 = g(z) = \\frac{1}{1 + e^{-z}} = P(y = 1|\\vec{x})$\n",
    "- If $a_1 = 0.71$, then $P(y = 1|\\vec{x}) = 0.29$\n",
    "- Loss Function = $-y\\log{(a_1)} - (1 - y) \\log{(a_2)}$\n",
    "    - Note that $a_2 = 1 - a_1$ which is why the second log contains $a_2$\n",
    "\n",
    "<u>Softmax Regression</u> ($M$ possible outputs):\n",
    "- $z_j = \\vec{w}_j \\cdot \\vec{x} + b_j$ where $j = 1, \\ldots, M$\n",
    "- $a_j = \\frac{e^{z_j}}{\\sum_{k = 1}^M(e^{z_k})} = P(y = j|\\vec{x})$\n",
    "\n",
    "<u>Parameters for Softmax Regression:</u> $w_1 \\ldots w_M$ and $b_1 \\ldots b_M$\n",
    "\n",
    "<u>Soft Max Loss Function:</u> $\\left\\{\\begin{array}{lr} -\\log{(a_1)}, & \\text{if } y^{(i)} = 1 \\\\ -\\log{(a_2)}, & \\text{if } y^{(i)} = 2 \\\\ \\vdots \\\\ -\\log{(a_M)}, & \\text{if } y^{(i)} = M \\end{array} \\right\\}$\n",
    "\n",
    "We can train these parameters using gradient descent and that would give us our trained multiclass classification model.\n",
    "- If we would set $M = 2$, then the Softmax Regression model will reduce to the Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification (Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-22-00-02-35.png)\n",
    "\n",
    "- The only change in this neural network is that the output layer is based on the softmax activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to implement this neural network in Tensorflow:\n",
    "![](2022-07-22-00-12-54.png)\n",
    "- There is another way to implement this but apparently, this way is more numerically accurate as it reduces round-off errors\n",
    "    - Ex. In the other way, the output layer activation function is 'softmax' and from_logits != True\n",
    "- After applying the linear function, you can apply the softmax function to the output of the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Algorithm: Learning Algorithm faster than Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Algorithm = An algorithm similar to Gradient Descent that can automatically decide/alter its learning rate $\\alpha$\n",
    "- The algorithm sets a different $alpha$ for each parameter that you need to update\n",
    "- Then, if $w_j$ or $b$ keeps moving in the same direction, you increase $\\alpha_j$  \n",
    "    - ![](2022-07-22-01-27-17.png)\n",
    "- If $w_j$ or $b$ keeps oscillating, reduce $\\alpha_j$\n",
    "    - ![](2022-07-22-01-27-35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use this algorithm in code (where you need to set some initial learning rate):\n",
    "\n",
    "![](2022-07-22-01-28-45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Layer Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Dense Layer</u> (What we've been looking at)\n",
    "- Each neuron looks at all of the values in the input vector fed into that layer\n",
    "\n",
    "<u>Convolutional Layer</u>\n",
    "- Each neuron only looks at part of the values in the input vector fed into that layer\n",
    "- Why?\n",
    "    - Faster computation\n",
    "    - Need less training data\n",
    "    - Less prone to overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd6ec6a6081db90e3c169c3e6504ac022610349e3b3cfc7a85010bdd1e87fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
