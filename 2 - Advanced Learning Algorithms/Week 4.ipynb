{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# Outline of Notebook\n",
    "- ### Decision Trees\n",
    "- ### Decision Tree Learning Intuition\n",
    "- ### Decision Tree Learning\n",
    "- ### Modifications to Decision Trees\n",
    "- ### Tree Ensembles\n",
    "- ### When to use Decision Trees and Tree Ensembles?\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-23-15-20-27.png)\n",
    "\n",
    "<u>The job the decision tree learning algorithm is that out of all possible decision trees, to try to pick one that hopefully does well on the training set and cross-validation set, and eventually on the test set too.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Learning Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-23-16-25-23.png)\n",
    "\n",
    "<u>Maximizing Purity</u> = you want to try to get only one class on one side and the other class on the other side when splitting\n",
    "\n",
    "<u>Decision 2:</u> When do you stop splitting?\n",
    "- When a node is 100% one class\n",
    "- When splitting a node will results in the tree exceeding a maximum depth\n",
    "    - You might want to do this because if your tree gets too big, it may be prone to overfitting\n",
    "- When improvements in purity score are below a threshold (purity score not increasing much)\n",
    "- When number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Entropy</u> = measure of the impurity of a set of data\n",
    "- $p_1$ = fraction of examples that is one class (binary classification example)\n",
    "- $p_0$ = $1 - p_1$ = fraction of examples that is the other class (binary classification example)\n",
    "- Entropy function: $H(p_1) = -p_1\\log_2{p_1} - p_0\\log_2(p_0)$ (Note: $0 * \\log_2{0} = 0$, let's assume)\n",
    "\n",
    "    ![](2022-07-23-16-36-33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Choosing a Splitting Feature</u> = the feature that minimizes entropy = maximizes purity\n",
    "- Let's assume that we are trying to classify cats from dogs using the features Ear Shape, Face Shape, and Whiskers\n",
    "- Let's also assume that we originally have 5 cats and 5 dogs in our dataset\n",
    "- $p_1$ = the fraction of examples that are cats in a dataset\n",
    "\n",
    "    ![](2022-07-23-16-54-45.png)\n",
    "    - Above in the picture, we found the entrophy of each split for each feature\n",
    "    - Then, we computed the weighted average of the left and right datasets for each split\n",
    "        - Computing the weighted average is important because a small dataset with high impurity is better than a large dataset with high impurity\n",
    "    - Then, we subtracted the weighted average from the previous entropy (since this is the root node that we are trying to decide, the previous entropy was just 1 because $p_1 = 0.5$ as there were 5 cats out of 10 data points)\n",
    "    - What this finally measures is the reduction in entropy that occurs and we would choose the split that reduces entrophy the most\n",
    "        - We go through the hurdle of calculating the reduction in entropy because one way to decide how to stop splitting is if the reduction in entropy is too small and if you're just making your tree prone to overfitting by increasing its size\n",
    "\n",
    "<u>Information Gain</u> = the calculated reduction in entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>The Final Algorithm:</u>\n",
    "- Start with all examples at the root node\n",
    "- Calculate information gain for all possible features, and pick the one with the highest information gain\n",
    "- Split the dataset according to selected feature, and create left and right branches of the tree splitting data and putting them into those branches as well\n",
    "- Keep repeating splitting process until stopping criteria is met:\n",
    "    - When a node is 100% one class\n",
    "    - When splitting a node will results in the tree exceeding a maximum depth\n",
    "        - You might want to do this because if your tree gets too big, it may be prone to overfitting\n",
    "    - When improvements in purity score are below a threshold (purity score not increasing much)\n",
    "    - When number of examples in a node is below a threshold\n",
    "\n",
    "NOTE: To decide your maximum depth, you can use the same path we used to decide the degree of polynomials we should try for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifications to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a feature that has 3 unique values instead of 2\n",
    "- In this case, you can use One Hot Encoding to change the one feature to 3 additional features, each having binary values (0 or 1)\n",
    "- By One Hot Encoding these features, you convert them to numbers which allows you to feed the data into Logistic Regression or Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a feature that is continuous, like the weight of an animal\n",
    "- In this case, the Decision Tree algorithm will split the weight feature on a number that results in the greatest information gain\n",
    "    - Ex. the weight feature can split on 8 kgs: anything below 8 kgs goes to the left branch and another equal to or above 8 kgs goes to right branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Ensembles (Multiple Decision Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantage of a Single Decision Tree = the tree can be highly sensitive to small changes in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-23-17-44-55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>How to build Tree Ensembles:</u>\n",
    "\n",
    "Given training set of size $m$\n",
    "\n",
    "Given the number of trees you want in your Tree Ensemble $B$\n",
    "\n",
    "For $b = 1$ to $B$:\n",
    "- Use sampling <u>with</u> replacement to create a new training set of size $m$ (note that training examples may be repeated)\n",
    "- Train a decision tree on the new dataset\n",
    "\n",
    "For the value of $B$, most recommend any value from 64 to 228\n",
    "\n",
    "This type of Tree Ensemble is called a Bagged Tree Ensemble because of the sampling with replacement\n",
    "\n",
    "However, there are other like the <u>Random Forest Tree Ensemble</u>:\n",
    "- The Random Forest Algorithm has one extra addition to the Bagged Tree Algorithm\n",
    "- At each node, when choosing a feature to use to split, if $n$ features are available, pick a random subset of $k < n$ features and allow the algorithm to only choose from that subset of features\n",
    "- If $n$ is large, then you usually choose $k = \\sqrt{n}$\n",
    "\n",
    "<u>Usually the Random Forest Algorithm works better than the Bagged Tree Algorithm as well as a Single Decision Tree</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's another widely known algorithm called <u>XGBoost</u>:\n",
    "- Given training set of size $m$\n",
    "- For $b = 1$ to $B$:\n",
    "    - Use sampling with replacement to create a new training set of size $m$\n",
    "        - But instead of picking from all examples with equal $\\frac{1}{m}$ probability, make it more likely to pick examples that the previously trained trees misclassify\n",
    "    - Train a decision tree on the new dataset\n",
    "\n",
    "- XGBoost is fast and also has built in regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use Decision Trees and Tree Ensembles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees and Tree ensembles\n",
    "- Works well on tabular (structured) data\n",
    "- Not recommended for unstructured data (images, audio, text)\n",
    "- Small decision trees may be human interpretable\n",
    "- Main Advantage: Fast\n",
    "\n",
    "If you have decided to use Decision Trees:\n",
    "- Mostly, use XGBoost\n",
    "\n",
    "Decision Trees vs. Neural Networks\n",
    "- Neural Networks works well on all types of data, including structured and unstructured data\n",
    "- But, Neural Networks may be slower than a decision tree\n",
    "- However, Neural Networks allow you to work with transfer learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd6ec6a6081db90e3c169c3e6504ac022610349e3b3cfc7a85010bdd1e87fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
