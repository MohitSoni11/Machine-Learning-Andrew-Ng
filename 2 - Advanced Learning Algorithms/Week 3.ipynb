{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Regression:</u>\n",
    "- Split data into training data and test data (Usually 70% to 30% or 80% to 20%)\n",
    "- Then, train the model on the training data using gradient descent\n",
    "- Then, after learning the parameters, use the cost function (without the regularization term) to see the error on the test data\n",
    "- Also, you can find the error on the train data to compare to the test data\n",
    "\n",
    "<u>Classification</u>\n",
    "- Split data into training and test data (Usually 70% to 30% or 80% to 20%)\n",
    "- Then, train the model on the trainign data using gradient descent\n",
    "- Then, after learning the parameters, find the fraction of misclassified labels on the test data and that's your error\n",
    "- Also, you can find the fraction on the train data (train data error) to compare to the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Regression:</u> (Linear or Polynomial? and what degree?)\n",
    "- Split data into training, cross-validation, and test set (Usually 60%, 20%, 20%)\n",
    "- Train model on the training data using gradient descent\n",
    "- Then, after learning the parameters, use the cost function (without the regularization term) to find the error on all three sets of data\n",
    "- Do the last three steps for different degree polynomials (degree 1 to degree 10 let's say)\n",
    "- Choose the degree that results in the lowest cross-validation error\n",
    "\n",
    "<u>Classification:</u> (Which neural network architecture to try, which classification model to try?)\n",
    "- Split data into training, cross-validation, and test set (Usually 60%, 20%, 20%)\n",
    "- Train model on the training data using gradient descent\n",
    "- Then, after learning the parameters, find the fraction of misclassified labels on the three sets of data\n",
    "- Do the last three steps for all the different classification models\n",
    "- Choose the model/architecture that results in the lowest cross-validation error\n",
    "\n",
    "NOTE: You can't just look at the test set and compare them because apparently, the test set should be used only for seeing the accuracy/error of the model and should not be used for anything else. This makes it so that you haven't \"fit\" your model to the test set which is unfair.\n",
    "\n",
    "NOTE: In Machine Learning, it is considered best practice to use only the Training or Cross Validation Set to make decision about your model, such as fitting parameters or choosing the model architecture, and not look at the test set at all when making these decisions. Then, once you've come up with your final model, you then evaluate it on your test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Performance of a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J_{train}$ = error on training data\n",
    "\n",
    "$J_{cv}$ = error on cross-validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>High Bias (Underfitting):</u>\n",
    "- $J_{train}$ is high\n",
    "- $J_{cv}$ is high\n",
    "\n",
    "<u>High Variance (Overfitting):</u>\n",
    "- $J_{train}$ is low\n",
    "- $J_{cv}$ is high\n",
    "\n",
    "<u>Just Right:</u>\n",
    "- $J_{train}$ is low\n",
    "- $J_{cv}$ is low\n",
    "\n",
    "![](2022-07-22-15-08-28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Regularization</u>\n",
    "\n",
    "![](2022-07-22-16-12-08.png)\n",
    "\n",
    "How to find the regularization parameter $\\lambda$:\n",
    "- Try $\\lambda = 0$ and observe $J_{cv}$\n",
    "- Try $\\lambda = 0.01$ and observe $J_{cv}$\n",
    "- Try $\\lambda = 0.02$ and observe $J_{cv}$\n",
    "- Try $\\lambda = 0.04$ and observe $J_{cv}$\n",
    "\n",
    "    ...\n",
    "- Try $\\lambda = 10$ and observe $J_{cv}$\n",
    "- Finally, use the lambda that results in the lowest $J_{cv}$\n",
    "\n",
    "![](2022-07-22-16-22-37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Establish a Baseline Level of Performance</u>\n",
    "- What is the level of error you can reasonably hope to get to?\n",
    "    - Human level performance\n",
    "    - Competing algorithms performance\n",
    "    - Guess based on experience\n",
    "- This baseline will help you to see whether the training error/CV error is low or high\n",
    "    - Ex. Speech Recognition\n",
    "        - Human performance error = 10.6%\n",
    "        - Train error = 10.8%\n",
    "        - CV error = 14.8%\n",
    "        - When only seeing the train error, it may seem very high. However, when comparing it to human performance, it seems pretty good. Therefore, this model may actually be overfitting instead of underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Learning Curves</u>\n",
    "\n",
    "![](2022-07-22-15-58-29.png)\n",
    "- The x-axis plots the size of the training set\n",
    "- The y-axis plots the errors\n",
    "- With high bias, both error curves will flatten out and there will be a big gap between $J_{train}$ and the baseline performance\n",
    "- NOTE: With high bias, you should not try to get more training data because that doesn't help\n",
    "\n",
    "![](2022-07-22-16-03-45.png)\n",
    "- With high variance, both error curves will flatten but there will be a big gap between $J_{train}$ and $J_{cv}$\n",
    "- Also, human level performance may actually be above $J_{train}$\n",
    "- NOTE: With high variance, you should try to get more training data as it will help because $J_{cv}$ will keep coming down approaching the $J_{train}$ curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Error Analysis</u>\n",
    "\n",
    "Assume $N_cv = 5000$ = # of examples in cross validation set and your algorithm misclassifies 1000 of them\n",
    "- You can manually examine 100 or 200 examples and categorize them based on common traits\n",
    "- This analysis can tell you if you're missing some feature\n",
    "- This manual examination can also tell you about features that occur rarely so there's no point of spending a lot of time fixing them\n",
    "\n",
    "Note that Error Analysis is easier if the job can be done by a human (seeing if an email is spam or not) but it's harder if the job cannot be easily done by a human (see which ad a user will click on).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>What to try Next after the Diagnostics</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Variance (Overfitting):\n",
    "- Get more training examples\n",
    "- Try smaller sets of features\n",
    "- Try increasing $\\lambda$\n",
    "\n",
    "High Bias (Underfitting):\n",
    "- Try getting additional features\n",
    "- Try adding polynomial features\n",
    "- Try decreasing $\\lambda$\n",
    "- NOTE: Reducing training set size doesn't help\n",
    "\n",
    "How to add more data?\n",
    "- Add specific data\n",
    "    - Let's say error analysis tells you that 50 misclassified emails are from pharma companies\n",
    "    - Then, you can add more pharma spam data into your training data so the model does better\n",
    "- Data Augmentation\n",
    "    - Modifying an existing training example to create a new training example\n",
    "    - The new training example should be representative of the types of noise or distortions that come up in the test set\n",
    "- Data Synthesis\n",
    "    - Creating a brand new training example\n",
    "    - Again, like Data Augmentation, the new training example should be representative of examples in the test set\n",
    "- Transfer Learning\n",
    "    - For when you cannot make/collect more data\n",
    "    - Train your model on a different task with a large dataset\n",
    "    - Then, use the trained model and train it again with the intended task\n",
    "    - The intuition is that by pretraining your model, the parameters start off at a much better place, and that can result in the final model to be good\n",
    "    - Note that there are many already trained neural networks online and you can use that to perform Transfer Learning\n",
    "    - Note that the pretrained model should have the same input as your final model because then pretraining won't do much good\n",
    "        - Ex. You can pretrain on Cats & Dog images and use that for training on Handwritten digit images\n",
    "\n",
    "    ![](2022-07-22-17-53-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Bias/Variance for Neural Networks</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-22-16-37-26.png)\n",
    "- Training a bigger neural network does reduce bias but it can get computationally expensive\n",
    "- Also, sometimes you may not be able to get more data\n",
    "\n",
    "<u>A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately.</u>\n",
    "- Regularizing a neural network in code (in the case below, $\\lambda$ is 0.01):\n",
    "\n",
    "    ![](2022-07-22-16-42-07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Development Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-22-16-49-57.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Cycle of a Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define Scope of Project\n",
    "    - Ex. Speech Recognition for Voice Search\n",
    "2. Collect Data\n",
    "3. Refine Data\n",
    "4. Train model\n",
    "5. Training, error analysis & iterative improvement\n",
    "6. Deploy in Production and maintain system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Confusion Matrix</u>\n",
    "\n",
    "![](2022-07-22-18-41-04.png)\n",
    "\n",
    "Let's say 1 means that the patient has the rare disease and 0 means the patient doesn't\n",
    "\n",
    "<u>Error metric:</u> We use different error metrics called Precision and Recall for imbalanced data\n",
    "- Precision = of all patients where we predicted y = 1, what fraction actually have the rare disease? $$\\frac{\\text{True positives}}{\\text{Total Predicted Positive}} = \\frac{\\text{True positives}}{\\text{True pos + False pos}}$$\n",
    "- Recall = of all patients that actually have the rare disease, what fraction did we correctly detect as having it? $$\\frac{\\text{True positives}}{\\text{Total Actual Positive}} = \\frac{\\text{True positives}}{\\text{True pos + False neg}}$$\n",
    "- With these metrics, you can see if your model is just printing the majority class all the time because if it is, then the True Positives will be 0 making Precision = undefined and Recall = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Precision and Recall Trade-off</u>\n",
    "\n",
    "Usually, we want both Precision and Recall to be high but that cannot always happen\n",
    "\n",
    "High Threshold\n",
    "- Let's say that a disease is not very lethal but its treatment is very expensive\n",
    "- In this case, you may want to raise your threshold so you predict 1 whenever you have a 70% chance or higher\n",
    "- In this case, precision increases and recall decreases\n",
    "\n",
    "Low Threshold\n",
    "- Let's say that a disease's treatment is not expensive but if not treated, it can be lethal\n",
    "- In this case, you may want to lower your threshold so you predict 1 whenever you have a 30% chance or higher\n",
    "- In this case, precision decreases and recall increases\n",
    "\n",
    "![](2022-07-22-18-57-54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>F1-Score:</u> the harmonic mean of the precision (P) and recall (R) -- gives emphasis to whichever value is lower $$\\frac{1}{\\frac{1}{2}(\\frac{1}{P} + \\frac{1}{R})}$$\n",
    "- This is because it turns out that if an algorithm has very low precision or recall, it is probably not very useful\n",
    "- The F1 score gives a way to trade-off the precision and the recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd6ec6a6081db90e3c169c3e6504ac022610349e3b3cfc7a85010bdd1e87fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
