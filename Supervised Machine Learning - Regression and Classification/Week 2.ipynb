{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_j$ = $j^{th}$ feature (a column in the dataset)\n",
    "\n",
    "$m$ = number of features\n",
    "\n",
    "$\\vec{x}^{(i)}$ = features of $i^{th}$ training example (a row in the dataset)\n",
    "\n",
    "$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example\n",
    "\n",
    "$y^{(i)}$ = $i^{th}$ house's price in $1000's\n",
    "\n",
    "$N$ = # of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous Model: $f_{w, b}(x) = wx + b$\n",
    "\n",
    "<u>Model for Multiple Features:</u> $f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b$\n",
    "\n",
    "- Where $\\vec{w} = \\begin{bmatrix} w_1, w_2, \\ldots, w_m \\end{bmatrix}$ and $\\vec{x} = \\begin{bmatrix} x_1, x_2, \\ldots, x_m \\end{bmatrix}$\n",
    "- The dot stands for Dot Product\n",
    "\n",
    "<u>Parameters of the New Model:</u> $\\begin{bmatrix} w_1, w_2, \\ldots, w_m \\end{bmatrix}$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>New Cost Function:</u> $J(\\vec{w}, b) = \\frac{1}{2N}\\sum_{i = 1}^{N}((\\vec{w} \\cdot \\vec{x}^{(i)} + b) - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Multiple Feature Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $w_1 = w_1 - \\alpha\\frac{\\partial}{\\partial w_1}J(\\vec{w}, b) \\ldots w_m = w_m - \\alpha\\frac{\\partial}{\\partial w_m}J(\\vec{w}, b)$\n",
    "    - We need to update all the different $w$ parameters we have from $w_1$ to $w_m$\n",
    "- $b = b - \\alpha\\frac{\\partial}{\\partial b}J(\\vec{w}, b)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Regression Work Well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Feature Engineering</u> = using intuition to design new features, by transforming or combining original features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of just dealing with the features you have, you can create new features from the data you have that make more sense\n",
    "- For example, let's say you have a feature for the width of a plot and a feature for the length of a plot to predict the plot's price\n",
    "- Even though you can use those features, you can create a third feature which is the area of the plot and this may be more helpful in predicting the price of the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vectorization of code reduces the amount of code you have to write as well as decreases the run time of the code\n",
    "- Run time is decreased because unlike a for loop, all the operations in vectorized code are done at once\n",
    "- In for loops, however, you perform operations one step at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952\n"
     ]
    }
   ],
   "source": [
    "# Without vectorization\n",
    "w = np.array([1, 2, 65, 3, 6, 42, 4, 6, 3])\n",
    "x = np.array([6, 3, 2, 5, 63, 5, 6, 3, 55])\n",
    "f = 0\n",
    "for j in range(0, len(w)):\n",
    "    f = f + w[j] * x[j]\n",
    "    \n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952\n"
     ]
    }
   ],
   "source": [
    "# With Vectorization\n",
    "# In this case, both vectors are multiplied in parallel rather than step by step in the for loop\n",
    "# Then, adding up the values is done by specialized hardware making it efficient rather than carrying out distinct additions\n",
    "f = np.dot(w, x)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to know Gradient Descent is Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot J (The Cost Function) and see if it is decreasing as the number of iterations of Gradient Descent increase and flattening at the end\n",
    "- If J increases after any iteration, alpha is choosen poorly (alpha is too big) or there is a bug in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Gradient Descent Run Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Feature Scaling</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having different features with considerable differences in their ranges can cause Gradient Descent to run slowly as the contour plot for the cost function is going to be skinny. However, if these features can be changed to have similar ranges of values, then the contour plot will be more circular and this will allow Gradient Descent to run faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you do feature scaling?\n",
    "- Assume one of your features $x_1$ ranges from 300 - 2000\n",
    "- Your second feature $x_2$ ranges from 0 to 5\n",
    "\n",
    "- $x_{1, scaled} = \\frac{x_1}{2000}$\n",
    "- $x_{2, scaled} = \\frac{x_2}{5}$\n",
    "\n",
    "- By doing this, your ranges for both features are:\n",
    "    - $0.15 \\leq x_{1, scaled} \\leq 1$\n",
    "    - $0 \\leq x_{2, scaled} \\leq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other types of feature scaling such as mean normalization and z-score normalization but I have not included it in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature scaling, acceptable ranges are $-3 \\leq x_j \\leq 3$ or $-0.3 \\leq x_j \\leq 0.3$ but you should try to aim for $-1 \\leq x_j \\leq 1$\n",
    "- $0 \\leq x_j \\leq 3$ ---- okay, no rescaling\n",
    "- $-2 \\leq x_j \\leq 0.5$ ---- okay, no rescaling\n",
    "- $-100 \\leq x_j \\leq 100$ ---- too large, rescale\n",
    "- $-0.001 \\leq x_j \\leq 0.001$ ---- too small, rescale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Choosing the Learning Rate</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the Cost Function (J) by the number of iterations of Gradient Descent\n",
    "- If J ever increases after an iteration of Gradient Descent, then alpha is probably too big and you should use a smaller alpha\n",
    "    - Note that if J increases, then it may not be a problem with alpha, your code may be wrong\n",
    "    - To check if your code is wrong, choose a very small alpha and see if J is decreasing on every iteration (if it isn't, your code is wrong)\n",
    "- If Gradient Descent is taking a very large number of iterations to converge, then alpha may be too small\n",
    "\n",
    "\n",
    "- Andrew Strategy: Try 0.001, then 0.003, then 0.01, then 0.03... keep multiplying by about 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not all data can be represented by lines, you may also need to fit curves\n",
    "- For example, a quadratic curve: $f_{w, b}(x) = w_1x + w_2x^2 + b$\n",
    "- Or a cubic function: $f_{w, b}(x) = w_1x + w_2x^2 + w_3x^3 + b$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "425927320cfd38c1f4d04dab592055e2380257b6c8d6532762caec5aa72862cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
