{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# Outline of Notebook\n",
    "- ### ML Definitions/Intro\n",
    "- ### Linear Regression with One Feature\n",
    "- ### Gradient Descent\n",
    "- ### Properties of Gradient Descent / Why is works?\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML = gives the computers the ability to learn without being explicitly programmed\n",
    "\n",
    "Supervised Learning = algorithms that learn x to y or input to output mappings\n",
    "- Give \"right answers\"\n",
    "- Regression - predicts a number from infinite possibilities\n",
    "- Classification - predicts a label/category\n",
    "\n",
    "Unsupervised Learning\n",
    "- Don't give \"right answers\", only input\n",
    "- Clustering - assigns input to groups\n",
    "- Anomaly Detection - finds unusual data points\n",
    "Dimensionality Reduction - compresses data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: Fit Line to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-19-14-08-59.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we are trying to fit a line through a dataset which has one feature - size of house in square feet - and the target variable is the price of the house in $1000's. Below is an example of that dataset:\n",
    "\n",
    "![](2022-07-19-14-37-49.png)\n",
    "\n",
    "$x^{(i)}$ = i'th house's size if square feet in training data\n",
    "\n",
    "$y^{(i)}$ = i'th house's price in $1000's in training data\n",
    "\n",
    "$(x^{(i)}, y^{(i)})$ = a data point in the training dataset / on the graph above\n",
    "\n",
    "$N$ = # of training examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Model:</u> $f_{w, b}(x) = wx + b$\n",
    "- <u>Parameters:</u> $w, b$\n",
    "- Parameters of a model = variables adjusted during training to improve model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function = how good the model is doing with its current parameters\n",
    "- In this case, our cost function is the Least Squares Cost Function: $$\\frac{1}{2N}\\sum_{i = 1}^{N}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "- <u>Final Cost Function:</u> $$J(w, b) = \\frac{1}{2N}\\sum_{i = 1}^{N}((wx^{(i)} + b) - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Our goal is to choose $w, b$ that minimizes $J(w, b)$</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start with some $w, b$\n",
    "- w is updated: $w = w - \\alpha\\frac{\\partial}{\\partial w}J(w, b)$\n",
    "- b is updated: $b = b - \\alpha\\frac{\\partial}{\\partial b}J(w, b)$\n",
    "- $\\alpha$ = learning rate (always between 0 and 1)\n",
    "- $w, b$ must be updated simultaneously; otherwise if w is updated first, J(w, b) changes so b is updated incorrectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think of Gradient Descent is: $\\begin{bmatrix} w \\\\ b \\end{bmatrix}$ - $\\alpha$ * Jacobian of $J(w, b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Gradient Descent to calculate the optimum values for the parameters $w, b$ giving us our function f -- $f(x) = wx + b$ -- with which we can predict house prices based on the size of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of Gradient Descent / Why it works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We have used a simplified view of J to understand the concept. We have eliminated the parameter b in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-19-15-26-55.png)\n",
    "\n",
    "If we start at point a, the derivative of $J$ is positive meaning $w$ will decrease towards min (which is what we want)\n",
    "\n",
    "If we start at point b, the derivative of $J$ is negative meaning $w$ will increase towards min (which is what we want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-19-15-07-09.png)\n",
    "\n",
    "If $\\alpha$ is too small, Gradient Descent will take very small steps so it will take a very long time to converge.\n",
    "\n",
    "If $\\alpha$ is too large, Gradient Descent will take too big steps and it may keep going away from the min so Gradient Descent may not even work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent works even when $\\alpha$ is constant because the derivative term decreases as you get closer to the min. Therefore, gradient descent automatically takes smaller steps as it gets closer and closer to a minimum and eventually stops because at the minimum, the derivative term is going to evaluate to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>PROBLEM:</u> Gradient Descent can find local min rather than global min depending on starting parameters. Doesn't happen with Linear Regression because there's always going to be 1 minimum, the global minimum as the cost function for Linear Regression is a convex function (a function with only one minimum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd6ec6a6081db90e3c169c3e6504ac022610349e3b3cfc7a85010bdd1e87fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
