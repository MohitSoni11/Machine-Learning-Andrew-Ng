{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# Outline of Notebook\n",
    "- ### Classification - Logistic Regression\n",
    "- ### Gradient Descent for Logistic Regression\n",
    "- ### Overfitting & Underfitting + Regularization\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Sigmoid Function</u>: $g(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "![](2022-07-20-12-45-15.png)\n",
    "\n",
    "You can verify the shape of $g(z)$:\n",
    "- $\\lim\\limits_{x \\to \\infty} g(z) = 1$\n",
    "- $\\lim\\limits_{x \\to -\\infty} g(z) = 0$\n",
    "- $g(0) = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the Linear Regression model: $f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b$\n",
    "\n",
    "Now, let's set $z = f_{\\vec{w}, b}(x)$\n",
    "\n",
    "And let's input z back into the Sigmoid Function: $g(z) = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Model for Logistic Regression:</u> $f_{w, b}(\\vec{x}) = g(\\vec{w} \\cdot \\vec{x} + b) = \\frac{1}{1 + e^{-(\\vec{w} \\cdot \\vec{x} + b)}}$\n",
    "- Remember that $f$ is always going to return a number between 0 and 1 since the sigmoid function is applied to it\n",
    "\n",
    "<u>Interpreting Output of Model:</u> \"probability\" that class is 1\n",
    "- Ex. $x$ is \"tumor size\", $y$ is 0 (benign) or 1 (malignant)\n",
    "- If $f$ outputs 0.1, then there is a 10% chance that the current tumor is malignant\n",
    "- If $f$ outputs 0.5, then there is a 50% chance that the current tumor is malignant\n",
    "- If $f$ outputs 0.8, then there is a 80% chance that the current tumor is malignant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Decision Boundary:</u> the dividing line between one class and another\n",
    "- The model predicts 1 whenever $z \\geq 0$ as seen in the plot above\n",
    "- Therefore, the model predicts 1 whenever $\\vec{w} \\cdot \\vec{x} + b \\geq 0$\n",
    "- As a result, the decision boundary is whenever $\\vec{w} \\cdot \\vec{x} + b = 0$\n",
    "\n",
    "![](2022-07-20-14-11-44.png)\n",
    "- The decision boundary for $f$ (with 2 features) when the parameters $w_1 = 1, w_2 = 1, b = -3$\n",
    "\n",
    "![](2022-07-20-14-15-06.png)\n",
    "- A non-linear decision boundary for another $f$ with two features but a different $z$ where $w_1 = 1, w_2 = 1, b = -1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Loss Function for Logistic Regression:</u> $L(f_{\\vec{w}, b}(\\vec{x}^{(i)}, y^{(i)})) = \\left\\{\\begin{array}{lr} -\\log{(f_{w, b}(\\vec{x}^{(i)}))}, & \\text{if } y^{(i)} = 1 \\\\ -\\log{(1 - f_{w, b}(\\vec{x}^{(i)}))}, & \\text{if } y^{(i)} = 0 \\end{array} \\right\\}$\n",
    "\n",
    "<u>Simplified Loss Function:</u> $L(f_{\\vec{w}, b}(\\vec{x}^{(i)}, y^{(i)})) = -y^{(i)}\\log(f_{\\vec{w}, b}(\\vec{x}^{(i)})) - (1 - y^{(i)})\\log(1 - f_{\\vec{w}, b}(\\vec{x}^{(i)}))$\n",
    "\n",
    "<u>Cost Function for Logistic Regression:</u> $J(\\vec{w}, b) = \\frac{1}{N}\\sum_{i = 1}^{N}(L(f_{\\vec{w}, b}(\\vec{x}^{(i)}, y^{(i)})))$\n",
    "- Cannot use the same Cost Function as Linear Regression because then the cost function is then not convex for Logistic Regression when plotted (meaning it has several local minimums other than the global minimum) which makes it hard for Gradient Descent to converge to the global minimum\n",
    "- Also, this Cost Function makes wrong classifications very expensive because if $y^{(i)} = 1$ and your predicted value is 0 with 100% confidence, then the Cost Function will go to infinity. Same happens when $y^{(i)} = 0$ and your predicted value is 1 with 100% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform simultaneous updates {\n",
    "- $w_j = w_j - \\alpha\\frac{\\partial}{\\partial w_j}J(\\vec{w}, b)$\n",
    "    - Where $j = 1 \\ldots m$\n",
    "- $b = b - \\alpha\\frac{\\partial}{\\partial b}J(\\vec{w}, b)$\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting & Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-20-15-46-45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-20-15-50-40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Combating Overfitting</u>\n",
    "- Gather more training data\n",
    "- Use fewer features\n",
    "- Regularization = decrease the size of your parameters $w_1 \\ldots w_m$\n",
    "    - Benefits: It allows you to keep all your features but it restricts some from having an overly large affect\n",
    "    - It is basically like having all the features but still having a simpler model as the additional features don't have a large effect\n",
    "    - Regularizing b does not really matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Regularization in Practice:</u>\n",
    "- Assume you overfit the model $w_1x + w_2x^2 + w_3x^3 + w_4x^4 + b$ to data\n",
    "- To reduce overfitting, you want to make $w_3, w_4$ really small\n",
    "- You can do this by altering your cost function: $J(\\vec{w}, b) = \\frac{1}{2N}\\sum_{i = 1}^{N}(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})^2 + 1000w_3^2 + 1000w_4^2$\n",
    "    - The addition at the end would penalize the model if $w_3, w_4$ are large\n",
    "    - Therefore, the model would choose small values for $w_3, w_4$\n",
    "\n",
    "<u>How Regularization is Usually Done:</u>\n",
    "- If you don't know which feature to penalize and let's say you have a 100 features, then you penalize all of them a little bit\n",
    "- Your Linear Regression Cost Function changes to $J(\\vec{w}, b) = \\frac{1}{2N}\\sum_{i = 1}^{N}(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2N}\\sum_{j = 1}^{m} (w_j)^2$\n",
    "    - $\\lambda$ is called the regularization parameter (similar to learning rate $\\alpha$, you also have to choose a number for $\\lambda$)\n",
    "    - If $\\lambda$ is too small, then the model may overfit\n",
    "    - If $\\lambda$ is too big, then the model may underfit as all the parameters will be very close to 0, except b\n",
    "\n",
    "![](2022-07-20-18-24-56.png)\n",
    "\n",
    "- Your Cost Function for Logistic Regression looks similar:\n",
    "\n",
    "![](2022-07-20-23-27-16.png)\n",
    "\n",
    "- Note that in this picture, $m$ represents the # of datapoints and $n$ represents the # of features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd6ec6a6081db90e3c169c3e6504ac022610349e3b3cfc7a85010bdd1e87fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
