{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# Outline of Notebook\n",
    "- ### Reinforcement Learning\n",
    "- ### Deep Reinforcement Learning\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Intuition</u>\n",
    "- To train a dog, you can't demonstrate anything. Instead, when it does something good, you say \"Good Dog\" and when it does something bad, you say \"Bad Dog\"\n",
    "- You hope that the dog will learn to get better at doing the things that make us say \"Good Dog\"\n",
    "- This is how Reinforcement Learning Works\n",
    "- Reinforcement learning is based on a reward function which decides how well something is doing\n",
    "- For example, when flying a helicopter, you may want it to fly automatically and for that, you may tell the helicopter it is doing good if it is stable and if it crashes, you may somehow tell it that that was bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Return in Reinforcement Learning</u>\n",
    "\n",
    "![](2022-07-29-22-49-31.png)\n",
    "- Let's say you are using Reinforcement learning to decide where a rover should go (State 1, 2, 3, 4, 5, or 6)\n",
    "- The reward at State 1 = 100, the reward at State 6 = 40, and the reward at the other states are 0\n",
    "\n",
    "Let's assume that the rover goes from State 4 to State 1. Steps taken:\n",
    "- State 4 = 0 reward\n",
    "- State 3 = 0 reward\n",
    "- State 2 = 0 reward\n",
    "- State 1 = 100 reward\n",
    "\n",
    "Return = $0 + 0(0.9) + 0(0.9)^2 + 100(0.9)^3$ where $0.9$ = discount factor\n",
    "- What the discount factor does is that it penalizes the rover for going further. This is because if you can get a $5 bill right where you are, or you can get a $10 bill by traveling 30 minutes away, you may stick with the $5 bill because you don't want to travel. Here also, if you take less steps, you get a higher reward than if you take more steps because of the discount factor.\n",
    "\n",
    "<u>Generalized Return</u> \n",
    "$$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\ldots$$\n",
    "Where $R_i$ = reward and $\\gamma$ = discount factor ($\\gamma < 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Formalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do is find a function that takes in the state of the rover and returns what action we should take so as to maximize the return.\n",
    "\n",
    "The function is called a policy and is written $\\pi(s) = a$\n",
    "- This function is saying that given a state, the policy tells us an action that maximizes the return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>State Action Value Function (Q-Function)</u>\n",
    "\n",
    "$Q(s, a)$ = Gives the Return if you\n",
    "- start in state $s$\n",
    "- take action $a$ (once)\n",
    "- then behave optimally after that (not very clear but will clarify later)\n",
    "\n",
    "![](2022-07-29-23-45-07.png)\n",
    "\n",
    "Based on the Q-Function, you can pick actions that you want to do:\n",
    "- The best possible return from state $s$ is the maximum of the Q-Function for all actions that can be taken in the state $s$\n",
    "- The best possible action in state $s$ is the action $a$ that gives the maximum of the Q-Function for all actions that can be taken in the state $s$\n",
    "\n",
    "![](2022-07-29-23-48-31.png)\n",
    "- Here the optimal action from state 4 is to go left as the Q-Function value is greater\n",
    "- Similarly, the optimal action from state 5 is to go right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Bellman Equation</u>\n",
    "\n",
    "Notation:\n",
    "- $s$ = current state\n",
    "- $R(s)$ = reward of current state\n",
    "- $a$ = current action\n",
    "- $s'$ = state you get to after taking action $a$\n",
    "- $a'$ = action that you take in state $s'$\n",
    "\n",
    "As a reminder: \n",
    "\n",
    "$Q(s, a)$ = Gives the Return if you\n",
    "- start in state $s$\n",
    "- take action $a$ (once)\n",
    "- then behave optimally after that\n",
    "\n",
    "Bellman Equation: $$Q(s, a) = R(s) + \\gamma\\max_{a'}(Q(s', a'))$$\n",
    "- This matches the Q definition above\n",
    "- R(s) stands for the reward you get right away\n",
    "- The second term represents the return from behaving optimally starting from state s'\n",
    "- Another way of writing $R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\ldots $\n",
    "- Think of it like recursion, where you're using Q in itself and if you keep substituting future Q equations into the previous one, you will get $R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\ldots $ \n",
    "\n",
    "![](2022-07-30-00-02-58.png)\n",
    "\n",
    "Here, let's say we want to calculate Q(2, to the right)\n",
    "\n",
    "$R(2) = 0$\n",
    "\n",
    "$\\gamma = 0.5$\n",
    "\n",
    "$\\max_{a'}(Q(3, a')) = 25$\n",
    "\n",
    "Thus, $Q(2, \\text{right}) = 0 + 0.5(25) = 12.5$ and this is the Q value we derived earlier for Q(2, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous States vs. Discrete States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above examples, we were using discrete states because the state could only be one of 6 possible values for the rover. \n",
    "\n",
    "However, more often, you will have many more variables to represent the state and those variables are going to be continuous. For example, if you are trying to program an autonomous truck, to quantify its state, you need a vector giving you information about its x-location, y-location, x-speed, y-speed, angle at which it is turned, and the change in the angle. All these variables are continuous because they can take on any number, not just 1, 2, 3 or something.\n",
    "\n",
    "Everything else, however, is the same, in terms of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-30-00-55-48.png)\n",
    "- We are feeding a vector into the neural network with the states of an object and the action that is taken\n",
    "- The neural network then spits out the Q value for the specific action and state of the object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-30-01-10-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinements to Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Changing Architecture of Neural Network</u>\n",
    "\n",
    "![](2022-07-30-01-12-37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd6ec6a6081db90e3c169c3e6504ac022610349e3b3cfc7a85010bdd1e87fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
