{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# Outline of Notebook\n",
    "- ### Clustering\n",
    "- ### K-Means Clustering Algorithm\n",
    "- ### Anomaly Detection\n",
    "- ### Developing and Evaluating an Anomaly Detection System\n",
    "- ### Anomaly Detection vs. Supervised Learning\n",
    "- ### Choosing what Features to Use for Anomaly Detection\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Applications of Clustering\n",
    "- Grouping similar news\n",
    "- Market segmentation\n",
    "- DNA Analysis\n",
    "- Astronomical Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Intuition</u>\n",
    "\n",
    "Assume you are trying to find 2 clusters for 2D data\n",
    "1. Choose two random starting points for the cluster centroids\n",
    "    - Red Cross = one cluster centroid\n",
    "    - Blue Cross = the other cluster centroid\n",
    "2. Label the datapoints closer to the Red Cross as red cluster points and do the same for the Blue Cross\n",
    "3. Recompute the cluster centroids by taking average of blue points for Blue cluster centroid and red points for red cluster centroid\n",
    "4. Keep repeating Steps 2-3 and if the cluster centroids stop changing, you know the algorithm has converged\n",
    "\n",
    "Corner Case: If there are 0 datapoints assigned to a cluster, the average of the 0 points is undefined so you would remove that cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Formalizing Algorithm</u>\n",
    "\n",
    "Randomly initialize $K$ cluster centroids $\\mu_1, \\mu_2, \\ldots, \\mu_K$\n",
    "- Note that each centroid should be a $m$-dimensional vector where $m$ = # of features\n",
    "\n",
    "Repeat {\n",
    "    \n",
    "$\\quad$ <i style = 'color: gray'># Assign points to cluster centroids</i>\n",
    "\n",
    "$\\quad$ for $i = 1$ to $m$:\n",
    "\n",
    "$\\quad\\quad c^{(i)}$ := index (from 1 to $K$) of cluster centroid closest to $x^{(i)}$\n",
    "\n",
    "$\\quad$ <i style = 'color: gray'># Move cluster centroids</i>\n",
    "\n",
    "$\\quad$ for $k = 1$ to $K$:\n",
    "\n",
    "$\\quad\\quad \\mu_{k}$ := average (mean) of points assigned to cluster $k$\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Cost Function:</u> $$J(c^{(i)}, \\ldots, c^{(n)}, \\mu_1, \\ldots, \\mu_K) = \\frac{1}{n}\\sum_{i = 1}^n ||x^{(i)} - \\mu_{c^{(i)}}||$$\n",
    "- $\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned\n",
    "\n",
    "The cost function is calculating the square distance of each datapoint to its assigned cluster. What we want to do is minimize that. And it turns out that the K-Means algorithm is a way to minimize this cost function.\n",
    "- What this Cost Function is saying is that if the datapoints that are assigned to each cluster are closer to their cluster, then the Cost will be lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Random Initialization of Centroids (Most common way)</u>\n",
    "- Choose $K < n$ where $n$ = # of datapoints you have\n",
    "- Randomly pick $K$ training examples\n",
    "- Set $\\mu_1, \\ldots, \\mu_K$ equal to these $K$ examples\n",
    "\n",
    "However, depending on where your centroids start, the clusters can change.\n",
    "![](2022-07-28-13-23-23.png)\n",
    "- Here, the top clustering looks the best\n",
    "- However, the other ones are not as good because they are local minimums for the cost function\n",
    "\n",
    "<u>One way to combat this is to choose different starting points for your centroids and run the clustering algorithm multiple times (50 - 1000). Then, choose the starting point that minimizes the cost function the most.</u>\n",
    "- Ex. Above in the pic, the top graph has clusters that are very close to the points that they are related to resulting in lower cost values\n",
    "- Ex. On the other hand, the other graphs have clusters that are farther from their points resulting in larger cost values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Choosing the Number of Clusters:</u>\n",
    "- Elbow Method\n",
    "\n",
    "  ![](2022-07-28-13-40-54.png)\n",
    "  - Intuition behind this is that after $K = 3$, the cost function is decreasing slowly so there's no point of choosing clusters greater than $K = 3$\n",
    "\n",
    "\n",
    "NOTE: What doesn't work is choosing the # of clusters that minimizes the value of $J$ because then you would just choose the largest amount of clusters because usually, and increase in clusters leads to a decrease in $J$\n",
    "\n",
    "![](2022-07-28-13-44-28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection (Finding Unusual Events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Anomaly Detection Intuition</u>\n",
    "- You take the \"normal\" data and feed it to the algorithm\n",
    "- Then, when giving a test example, the algorithm sees if the probability of the values of the features is less than a certain value $\\epsilon$\n",
    "- If it is less than $\\epsilon$, then that's an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Anomaly Detection 1-Feature Algorithm</u>\n",
    "- Let's say you have a dataset $x^{(1)}, x^{(2)}, \\ldots, x^{(n)}$\n",
    "- For this dataset, you want to approximate a Gaussian (Normal) Distribution, which has parameters $\\mu$ and $\\sigma^2$\n",
    "- Calculating $\\mu = \\frac{1}{n}\\sum_{i = 1}^n (x^{(i)})$\n",
    "- Calculating $\\sigma^2 = \\frac{1}{n}\\sum_{i = 1}^n (x^{(i)} - \\mu)^2$\n",
    "- Now, when you get a new point, you can see the probability of that point coming up using the Gaussian Distribution $p(x)$ you derived\n",
    "- If $p(x) < \\epsilon$, then that's an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Anomaly Detection m-Feature Algorithm</u>\n",
    "- Let's say you have a dataset $x^{(1)}, x^{(2)}, \\ldots, x^{(n)}$\n",
    "- Each example $x_i$ has $n$ features, therefore $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix}$\n",
    "- $p(x) = p(x_1; \\mu_1, \\sigma_1^2) * p(x_2; \\mu_2, \\sigma_2^2) * \\ldots * p(x_m; \\mu_m, \\sigma_m^2)$ = $p(x) = \\prod_{j = 1}^m p(x_j; \\mu_j, \\sigma_j^2)$\n",
    "    - Calculating $\\mu_1 = \\frac{1}{n}\\sum_{i = 1}^n (x_1^{(i)})$\n",
    "    - Calculating $\\sigma_1^2 = \\frac{1}{n}\\sum_{i = 1}^n (x_1^{(i)} - \\mu_1)^2$\n",
    "    - We multiply the probabilities because if $x_1$'s chance of being a value is $\\frac{1}{10}$ and $x_2$'s chance of being a value is $\\frac{1}{2}$, then the chance of both things happening is $\\frac{1}{10} * \\frac{1}{2} = \\frac{1}{20}$\n",
    "- Now, when you get a new point, you can see the probability of that point coming up using the Gaussian Distribution $p(x)$ you derived\n",
    "- If $p(x) < \\epsilon$, then that's an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Anomaly Detection Final Algorithm</u>\n",
    "1. Choose $n$ features $x_i$ that you think might be indicative of anomalous examples\n",
    "2. Fit parameters $\\mu_1, \\ldots, \\mu_m, \\sigma_1^2, \\ldots, \\sigma_n^2$\n",
    "    - $\\mu_j = \\frac{1}{n}\\sum_{i = 1}^n (x_j^{(i)})$\n",
    "    - $\\sigma_j^2 = \\frac{1}{n}\\sum_{i = 1}^n (x_j^{(i)} - \\mu_j)^2$\n",
    "3. Given new example $x$, compute $p(x)$\n",
    "    - $\\prod_{j = 1}^m p(x_j; \\mu_j, \\sigma_j^2)$ where $p(x)$ is the Gaussian Distribution\n",
    "4. Anomaly if $p(x) < \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-07-28-14-35-54.png)\n",
    "- Here, $p(x)$ is derived from the multiplication of the 2 gaussian distributions of both features $x_1, x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing and Evaluating an Anomaly Detection System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that you have been working with Aircraft engines for several years\n",
    "- You have collected 10,000 good (normal) engines\n",
    "- You have also collected 20 flawed (anomalous) engines\n",
    "\n",
    "Data Splitting\n",
    "- Training set: 6000 good engines (if some anomalous engines slipped in, that's fine)\n",
    "- CV: 2000 good engines (y = 0), 10 anomalous (y = 1)\n",
    "- Test: 2000 good engines (y = 1), 10 anomalous (y = 1)\n",
    "\n",
    "Now, you can train your algorithm on the training set and test it on the CV set\n",
    "- Then, you can observe how many anomalous engines the algorithm correctly flags\n",
    "- Based on that, if the algorithm set flags too many as anomalies, then $\\epsilon$ may be too big\n",
    "- Similarly, you can tune the parameters of the model this way to be able to optimize the algorithm for your purposes\n",
    "- Then, you can use the algorithm on your test set\n",
    "\n",
    "Alternative: When you have very few anomalistic examples (maybe instead of 20, you had 2)\n",
    "- Training Set: 6000 good engines\n",
    "- CV: 4000 good engines (y = 0), 2 anomalous (y = 1)\n",
    "- No test set\n",
    "- This, however, can result in overfitting if you fit your parameters too much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Algorithm Evaluation</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit model $p(x)$ on training set\n",
    "- On a CV/Test example $x$, predict \n",
    " \n",
    "$$y = \\begin{cases} \n",
    "      1 & \\text{if } p(x) < \\epsilon \\text{ (anomaly)}\\\\\n",
    "      0 & \\text{if } p(x) \\geq \\epsilon \\text{ (normal)}\n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "Possible Evaluation Metrics (Since the target variable is very imbalanced):\n",
    "- True positive, false positive, false negative, true negative\n",
    "- Precision/Recall\n",
    "- F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection vs. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly Detection\n",
    "- Very small number of positive examples (y = 1). Large number of negative (y = 0) examples\n",
    "- Many different \"types\" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we've seen so far. \n",
    "- Ex. Fraud can be in several different forms and it is pretty rare\n",
    "- Therefore, if your target variable is severely imbalanced and you want to look for some unusual property, away from normal things, you choose Anomaly Detection\n",
    "\n",
    "Supervised Learning\n",
    "- Large number of positive and negative examples\n",
    "- Enough positive examples for algorithm to get a sense of what positive examples are like, future positive examples likely to be similar to ones in training set\n",
    "- Ex. Spam emails classification which can come in several different forms but there are certain things that are similar in most spam emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing what Features to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Anomaly Detection, the selection of features is extremely important\n",
    "\n",
    "In Supervised learning, if you have a few additional features, then that is still ok because it can learn to ignore some features. However, in Anomaly Detection, it looks for values in features that are unusual and it cannot learn if a feature is useless or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Non-Gaussian Features\n",
    "    - Plot feature using histogram\n",
    "    - Transform features to be more Normally distributed by applying:\n",
    "        - $\\log{(x)}$\n",
    "        - $\\log{(x + c)}$\n",
    "        - $\\sqrt{x}$\n",
    "    - After you transform the features of the training set, make sure to apply the same transformations to the CV/test set\n",
    "2. Error Analysis\n",
    "    - Most common problem: $p(x)$ is comparable (say, both large) for normal and anomalous examples\n",
    "    - In this case, if you look at the example that your algorithm failed on, then you may be able to see something unusual that you didn't include\n",
    "    - For example, let's say you are trying to find fraud behavior, and your algorithm says a transaction is normal even though it is fraud. Then, you look at that algorithm and notice that the typing speed is insanely high which you didn't have in your algorithm. Therefore, you can add a feature for the typing speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd6ec6a6081db90e3c169c3e6504ac022610349e3b3cfc7a85010bdd1e87fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
